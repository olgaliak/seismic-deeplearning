# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.

# Pull request against these branches will trigger this build
pr:
- master
- staging
- contrib

# Any commit to this branch will trigger the build.
trigger:
- master
- staging
- contrib

jobs:

# partially disable setup for now - done manually on build VM
- job: setup
  timeoutInMinutes: 10
  displayName: Setup
  pool:
    name: deepseismicagentpool
  steps:
    - bash: |
        # terminate as soon as any internal script fails
        set -e

        echo "Running setup..."
        pwd
        ls
        git branch
        uname -ra

        # setup run environment
        ./scripts/env_reinstall.sh 

        # use hardcoded root for now because not sure how env changes under ADO policy
        DATA_ROOT="/home/alfred/data_dynamic"
        ./tests/cicd/src/scripts/get_data_for_builds.sh ${DATA_ROOT}

        # upload pre-processed data to AML build WASB storage - overwrites by default and auto-creates container name
        azcopy --quiet --recursive \
          --source ${DATA_ROOT}/dutch_f3/data --destination https://${BLOB_ACCOUNT_NAME}.blob.core.windows.net/${BLOB_CONTAINER_NAME}/data \
          --dest-key ${BLOB_ACCOUNT_KEY}
      env:
        BLOB_ACCOUNT_NAME: $(amlbuildstore)
        BLOB_CONTAINER_NAME: "amlbuild"
        BLOB_ACCOUNT_KEY: $(amlbuildstorekey)


- job: AML_pipeline_tests
  dependsOn: setup
  timeoutInMinutes: 20
  displayName: AML pipeline tests
  pool:
    name: deepseismicagentpool
  steps:
  - bash: |      
      source activate seismic-interpretation
      # TODO: add code which launches your pytest files ("pytest sometest" OR "python test.py")
      # data is in  $(amlbuildstore).blob.core.windows.net/amlbuild/data (container amlbuild, virtual folder data)
      # storage key is $(amlbuildstorekey)
      az --version
      az account show
      az login --service-principal -u $SPIDENTITY -p $SPECRET --tenant $SPTENANT
      az account set --subscription $SUB_ID
      mkdir .azureml
      cat <<EOF > .azureml/config.json
      {
        "subscription_id": "$SUB_ID",
        "resource_group": "$RESOURCE_GROUP",
        "workspace_name": "$WORKSPACE_NAME"
      }
      EOF
      pytest interpretation/tests/test_train_pipeline.py || EXITCODE=123
      exit $EXITCODE
      pytest
    env:
      SUB_ID: $(subscription_id)
      RESOURCE_GROUP: $(resource_group)
      WORKSPACE_NAME: $(workspace_name)
      BLOB_ACCOUNT_NAME: $(amlbuildstore)
      BLOB_CONTAINER_NAME: "amlbuild"
      BLOB_ACCOUNT_KEY: $(amlbuildstorekey)
      BLOB_SUB_ID: $(subscription_id)
      AML_COMPUTE_CLUSTER_NAME: "testcluster"
      AML_COMPUTE_CLUSTER_MIN_NODES: "1"
      AML_COMPUTE_CLUSTER_MAX_NODES: "8"
      AML_COMPUTE_CLUSTER_SKU: "STANDARD_NC6"
      SPIDENTITY: $(spidentity)
      SPECRET: $(spsecret)
      SPTENANT: $(sptenant)
    displayName: 'integration tests'

# - job: AML_short_pipeline_test
#   dependsOn: setup
#   timeoutInMinutes: 5
#   displayName: AML short pipeline test
#   pool:
#     name: deepseismicagentpool
#   steps:
#   - bash: |      
#       source activate seismic-interpretation
#       # TODO: OPTIONAL! Add a job which launches entire training pipeline for 1 epoch of training (train model for single epoch)
#       # if you don't want this then delete the entire job from this file
#       python interpretation/deepseismic_interpretation/azureml_pipelines/dev/kickoff_train_pipeline.py --experiment=DEV-train-pipeline-name --orchestrator_config=orchestrator_config="interpretation/deepseismic_interpretation/azureml_pipelines/pipeline_config.json"


